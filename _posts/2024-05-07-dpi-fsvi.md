---
layout: distill
title: "Bridging the Data Processing Inequality and Function-Space Variational Inference"
description: >-
  This blog post explores the interplay between the <i>Data Processing Inequality (DPI)</i>, a cornerstone concept in information theory, and <i>Function-Space Variational Inference (FSVI)</i> within the context of Bayesian deep learning. The DPI governs the transformation and flow of information through stochastic processes, and its unique connection to FSVI is employed to highlight FSVI's focus on Bayesian predictive posteriors over parameter space. The post examines various forms of the DPI, including the KL divergence based DPI, and provides intuitive examples and detailed proofs. It also explores the equality case of the DPI to gain a deeper understanding. The connection between DPI and FSVI is then established, showing how FSVI can measure a predictive divergence independent of parameter symmetries. The post relates FSVI to knowledge distillation and label entropy regularization, highlighting the practical relevance of the theoretical concepts. Throughout the post, theoretical concepts are intertwined with intuitive explanations and mathematical rigor, offering a comprehensive understanding of these complex topics. By examining these concepts in depth, the post provides valuable insights for both theory and practice in machine learning.
date: 2024-05-07
future: true
htmlwidgets: true

authors:
  - name: Andreas Kirsch
    url: "https://www.blackhc.net"
    affiliations:
      name: University of Oxford<sup>&ndash;2023</sup>

# must be the exact same name as your blogpost
bibliography: 2024-05-07-dpi-fsvi.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
  - name: "Background: Information-Theoretic Notation"
  - name: "Data Processing Inequality"
    subsections:
      - name: "Example: Image Processing Pipeline"
      - name: "Example: Supervised Learning"
      - name: "Example: Autoencoders"
      - name: "Proof of the DPI"
  - name: "ðŸ¥¬ Data Processing Inequality"
    subsections:
      - name: "Example: Comparing Image Distributions"
      - name: "Counter-Example: Bayesian Inference"
      - name: "Proofs of the ðŸ¥¬ DPI"
      - name: Overall Statement
  - name: "Other Data Processing Inequalities"
    subsections:
      - name: "Jensen-Shannon Divergence"
      - name: "JSD-DPI"
      - name: "Mutual Information"
  - name: "Function-Space Variational Inference"  
    subsections:
      - name: "Problem Setting & Notation"
      - name: "Chain Rule of the ðŸ¥¬ Divergence & DPI"
      - name: "Deriving the Functional ELBO"
      - name: "Choosing the \"Coreset\""
      - name: "Application to Continual Learning"
  - name: Comparison to FSVI in the Literature
  - name: The Equality Case and Equivalence Classes
    subsections:
      - name: "Equivalence Classes"
      - name: "Consistency"
      - name: "Equality & Symmetries"
      - name: "Predictive Prior"
      - name: "The FSVI Objective"
  - name: "Parameter Priors vs. Predictive Priors"
    subsections:
      - name: "Label Entropy Regularization"
      - name: "Knowledge Distillation"
  - name: Conclusion

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .box-note, .box-warning, .box-error, .box-important {
    padding: 15px 15px 15px 10px;
    margin: 20px 20px 20px 5px;
    border: 1px solid #eee;
    border-left-width: 5px;
    border-radius: 5px 3px 3px 5px;
  }
  d-article .box-note {
    background-color: #eee;
    border-left-color: #2980b9;
  }
  d-article .box-warning {
    background-color: #fdf5d4;
    border-left-color: #f1c40f;
  }
  d-article .box-error {
    background-color: #f4dddb;
    border-left-color: #c0392b;
  }
  d-article .box-important {
    background-color: #d4f4dd;
    border-left-color: #2bc039;
  }
  html[data-theme='dark'] d-article .box-note {
    background-color: #333333;
    border-left-color: #2980b9;
  }
  html[data-theme='dark'] d-article .box-warning {
    background-color: #3f3f00;
    border-left-color: #f1c40f;
  }
  html[data-theme='dark'] d-article .box-error {
    background-color: #300000;
    border-left-color: #c0392b;
  }
  html[data-theme='dark'] d-article .box-important {
    background-color: #003300;
    border-left-color: #2bc039;
  }
  html[data-theme='dark'] d-article blockquote {
    color: var(--global-text-color) !important;
  }
  html[data-theme='dark'] d-article summary {
    color: var(--global-text-color) !important;
  }
  d-article aside * {
    color: var(--global-text-color) !important;
  }
  d-article p {
    text-align: justify;
    text-justify: inter-word;
    -ms-hyphens: auto;
    -moz-hyphens: auto;
    -webkit-hyphens: auto;
    hyphens: auto;
  }
  d-article aside {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
    font-size: 90%;
  }
  d-article aside p:first-child {
      margin-top: 0;
  }
  d-article details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
  }
  d-article summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
    display: list-item;
  }
  d-article details[open] {
    padding: .5em;
  }
  d-article details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
  }
categories:
- Data Processing Inequality
- Information Theory
- Data Processing Inequality
- Information Theory
- Function-Space Variational Inference
- Parameter Equivalence Classes
- Entropy Regularization
- Label Entropy Regularization
---

