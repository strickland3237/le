@inproceedings{DBLP:conf/iclr/PressSL22,
  author       = {Ofir Press and
                  Noah A. Smith and
                  Mike Lewis},
  title        = {Train Short, Test Long: Attention with Linear Biases Enables Input
                  Length Extrapolation},
  booktitle    = {The Tenth International Conference on Learning Representations, ICLR
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=R8sQPpGCv0},
  timestamp    = {Tue, 27 Dec 2022 12:44:40 +0100},
  biburl       = {https://dblp.org/rec/conf/iclr/PressSL22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/naacl/DevlinCLT19,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {BERT: Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {4171--4186},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1423},
  doi          = {10.18653/V1/N19-1423},
  timestamp    = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      url={https://arxiv.org/abs/1907.11692},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ott2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}

@inproceedings{DBLP:conf/iclr/BaevskiA19,
  author       = {Alexei Baevski and
                  Michael Auli},
  title        = {Adaptive Input Representations for Neural Language Modeling},
  booktitle    = {7th International Conference on Learning Representations, ICLR 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=ByxZX20qFQ},
  timestamp    = {Thu, 25 Jul 2019 14:26:00 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/BaevskiA19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/RadfordKHRGASAM21,
  author       = {Alec Radford and
                  Jong Wook Kim and
                  Chris Hallacy and
                  Aditya Ramesh and
                  Gabriel Goh and
                  Sandhini Agarwal and
                  Girish Sastry and
                  Amanda Askell and
                  Pamela Mishkin and
                  Jack Clark and
                  Gretchen Krueger and
                  Ilya Sutskever},
  editor       = {Marina Meila and
                  Tong Zhang},
  title        = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle    = {Proceedings of the 38th International Conference on Machine Learning,
                  ICML 2021, 18-24 July 2021, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {139},
  pages        = {8748--8763},
  publisher    = {PMLR},
  year         = {2021},
  url          = {http://proceedings.mlr.press/v139/radford21a.html},
  timestamp    = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/RadfordKHRGASAM21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{press-wolf-2017-using,
    title = "Using the Output Embedding to Improve Language Models",
    author = "Press, Ofir  and
      Wolf, Lior",
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-2025",
    pages = "157--163",
    abstract = "We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.",
}

@inproceedings{DBLP:conf/iclr/MerityX0S17,
  author       = {Stephen Merity and
                  Caiming Xiong and
                  James Bradbury and
                  Richard Socher},
  title        = {Pointer Sentinel Mixture Models},
  booktitle    = {5th International Conference on Learning Representations, ICLR 2017,
                  Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2017},
  url          = {https://openreview.net/forum?id=Byj72udxe},
  timestamp    = {Thu, 25 Jul 2019 14:25:57 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/MerityX0S17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2101-00027,
  author       = {Leo Gao and
                  Stella Biderman and
                  Sid Black and
                  Laurence Golding and
                  Travis Hoppe and
                  Charles Foster and
                  Jason Phang and
                  Horace He and
                  Anish Thite and
                  Noa Nabeshima and
                  Shawn Presser and
                  Connor Leahy},
  title        = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  journal      = {CoRR},
  volume       = {abs/2101.00027},
  year         = {2021},
  url          = {https://arxiv.org/abs/2101.00027},
  eprinttype    = {arXiv},
  eprint       = {2101.00027},
  timestamp    = {Thu, 14 Oct 2021 09:16:12 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2101-00027.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{hoffmann2022training,
      title={Training Compute-Optimal Large Language Models},
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      url={https://arxiv.org/abs/2203.15556},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{wang-etal-2018-glue,
    title = "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@misc{chowdhery2022palm,
      title={PaLM: Scaling Language Modeling with Pathways},
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      url={https://arxiv.org/abs/2204.02311},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tay2022scaling,
      title={Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?},
      author={Yi Tay and Mostafa Dehghani and Samira Abnar and Hyung Won Chung and William Fedus and Jinfeng Rao and Sharan Narang and Vinh Q. Tran and Dani Yogatama and Donald Metzler},
      year={2022},
      url={https://arxiv.org/abs/2207.10551},
      eprint={2207.10551},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{portes2023mosaicbert,
      title={MosaicBERT: How to Train BERT with a Lunch Money Budget},
      author={Jacob Portes and Alexander R Trott and Sam Havens and DANIEL KING and Abhinav Venigalla and Moin Nadeem and Nikhil Sardana and Daya Khudia and Jonathan Frankle},
      booktitle={Workshop on Efficient Systems for Foundation Models @ ICML2023},
      year={2023},
      url={https://openreview.net/forum?id=WH1S0gonzR}
}

@inproceedings{lee-etal-2022-littlebird,
    title = "LittleBird: Efficient Faster & Longer Transformer for Question Answering",
    author = "Lee, Minchul  and
      Han, Kijong  and
      Shin, Myeong Cheol",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.352",
    doi = "10.18653/v1/2022.emnlp-main.352",
    pages = "5261--5277",
    abstract = "BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism. Longformer, ETC and BigBird addressed this issue and effectively solved the quadratic dependency problem. However we find that these models are not sufficient, and propose LittleBird, a novel model based on BigBird with improved speed and memory footprint while maintaining accuracy. In particular, we devise a more flexible and efficient position representation method based on Attention with Linear Biases(ALiBi). We also show that replacing the method of global information represented in the BigBird with pack and unpack attention is more effective. The proposed model can work on long inputs even after being pre-trained on short inputs, and can be trained efficiently reusing existing pre-trained language model for short inputs. This is a significant benefit for low-resource languages where large amounts of long text data are difficult to obtain. As a result, our experiments show that LittleBird works very well in a variety of languages, achieving high performance in question answering tasks, particularly in KorQuAD2.0, Korean Question Answering Dataset for long paragraphs.",
}

@inproceedings{scao2022what,
    title={What Language Model to Train if You Have One Million GPU Hours?},
    author={Teven Le Scao and Thomas Wang and Daniel Hesslow and Lucile Saulnier and Stas Bekman and M Saiful Bari and Stella Biderman and Hady Elsahar and Jason Phang and Ofir Press and Colin Raffel and Victor Sanh and Sheng Shen and Lintang Sutawika and Jaesung Tae and Zheng Xin Yong and Julien Launay and Iz Beltagy},
    booktitle={Challenges & Perspectives in Creating Large Language Models},
    year={2022},
    url={https://openreview.net/forum?id=rI7BL3fHIZq}
}

@article{DBLP:journals/corr/abs-2305-19466,
  author       = {Amirhossein Kazemnejad and
                  Inkit Padhi and
                  Karthikeyan Natesan Ramamurthy and
                  Payel Das and
                  Siva Reddy},
  title        = {The Impact of Positional Encoding on Length Generalization in Transformers},
  journal      = {CoRR},
  volume       = {abs/2305.19466},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.19466},
  doi          = {10.48550/ARXIV.2305.19466},
  eprinttype    = {arXiv},
  eprint       = {2305.19466},
  timestamp    = {Wed, 07 Jun 2023 15:37:30 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-19466.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{haviv-etal-2022-transformer,
    title = "Transformer Language Models without Positional Encodings Still Learn Positional Information",
    author = "Haviv, Adi  and
      Ram, Ori  and
      Press, Ofir  and
      Izsak, Peter  and
      Levy, Omer",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.99",
    doi = "10.18653/v1/2022.findings-emnlp.99",
    pages = "1382--1390",
    abstract = "Causal transformer language models (LMs), such as GPT-3, typically require some form of positional encoding, such as positional embeddings. However, we show that LMs without any explicit positional encoding are still competitive with standard models and that this phenomenon is robust across different datasets, model sizes, and sequence lengths. Probing experiments reveal that such models acquire an implicit notion of absolute positions throughout the network, effectively compensating for the missing information. We conjecture that causal attention enables the model to infer the number of predecessors that each token can attend to, thereby approximating its absolute position. Our findings indicate that causal LMs might derive positional awareness not only from the explicit positioning mechanism but also from the effects of the causal mask.",
}
